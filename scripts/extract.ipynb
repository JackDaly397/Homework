{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-storage-blob\n",
        "!pip install boto3\n",
        "!pip install google-cloud-storage\n",
        "!pip install sodapy"
      ],
      "metadata": {
        "id": "gicUEe1hOBDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdJ-2JsVG3c6"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "import boto3\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "from google.cloud import storage\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "s1 --> Gather --> Decompress --> Convert to Tabular --> Clean --> Reformat --> Consolidate --> Tranformation --> Load"
      ],
      "metadata": {
        "id": "fUvaAkSoJZ6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sodapy import Socrata\n",
        "import pandas as pd\n",
        "from sodapy import Socrata\n",
        "\n",
        "data_url='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set='vww9-qguh'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token='m93ORKHDVAEgZJ48OfuDNs5sI'   # The app token created in the prior steps\n",
        "client = Socrata(data_url,app_token)      # Create the client to point to the API endpoint\n",
        "# Set the timeout to 60 seconds\n",
        "client.timeout = 60\n",
        "# Retrieve the first 50000 res0ults returned as JSON object from the API\n",
        "# The SoDaPy library converts this JSON object to a Python list of dictionaries\n",
        "results = client.get(data_set, limit=50000, offset=1)\n",
        "# Convert the list of dictionaries to a Pandas data frame\n",
        "df = pd.DataFrame.from_records(results)\n",
        "# Save the data frame to a CSV file"
      ],
      "metadata": {
        "id": "fBej0kFlRabF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "YGU3P8p-RPNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "f7VSo4yWFHdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function\n",
        "\n",
        "import os\n",
        "import boto3\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from io import BytesIO, StringIO\n",
        "\n",
        "# Azure Functions\n",
        "def azure_upload_blob(connect_str, container_name, blob_name, data):\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"Uploaded to Azure Blob: {blob_name}\")\n",
        "\n",
        "def azure_download_blob(connect_str, container_name, blob_name):\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    download_stream = blob_client.download_blob()\n",
        "    return download_stream.readall()"
      ],
      "metadata": {
        "id": "NuuN9M7tSSVn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sodapy import Socrata\n",
        "import pandas as pd\n",
        "from sodapy import Socrata\n",
        "\n",
        "data_url='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set='vww9-qguh'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token='m93ORKHDVAEgZJ48OfuDNs5sI'   # The app token created in the prior steps\n",
        "client = Socrata(data_url,app_token)      # Create the client to point to the API endpoint\n",
        "# Set the timeout to 60 seconds\n",
        "client.timeout = 60\n",
        "# Retrieve the first 2000 results returned as JSON object from the API\n",
        "# The SoDaPy library converts this JSON object to a Python list of dictionaries\n",
        "results = client.get(data_set, limit=2000)\n",
        "# Convert the list of dictionaries to a Pandas data frame\n",
        "df_raw = pd.DataFrame.from_records(results)\n",
        "df_raw.dropna()\n",
        "df_raw.shape\n",
        "df_raw.info()"
      ],
      "metadata": {
        "id": "8pwg7j8Sgru7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_raw.copy()"
      ],
      "metadata": {
        "id": "2exf8qt-Lemx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chunking data from API into list"
      ],
      "metadata": {
        "id": "wCjjD6utNLrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sodapy import Socrata\n",
        "import pandas as pd\n",
        "data_url='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set='vww9-qguh'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token='m93ORKHDVAEgZJ48OfuDNs5sI'   # The app token created in the prior steps\n",
        "client = Socrata(data_url, app_token)\n",
        "client.timeout = 60\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 50000\n",
        "offset = 0\n",
        "dfs = []\n",
        "\n",
        "while True:\n",
        "    # Retrieve data for the current chunk\n",
        "    results = client.get(data_set, limit=chunk_size, offset=offset)\n",
        "\n",
        "    # If there are no more results, break out of the loop\n",
        "    if not results:\n",
        "        break\n",
        "\n",
        "    # Convert the chunk to a DataFrame and append it to the list of DataFrames\n",
        "    df_chunk = pd.DataFrame.from_records(results)\n",
        "    dfs.append(df_chunk)\n",
        "\n",
        "    # Increment the offset for the next chunk\n",
        "    offset += chunk_size\n",
        "\n"
      ],
      "metadata": {
        "id": "TGx3lO9nVqKB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating connection to Azure and uploading chunks into Azure Blob"
      ],
      "metadata": {
        "id": "hU50TeROXrbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from io import StringIO\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "# Assuming you already have the DataFrame chunks stored in 'dfs' list\n",
        "#specify the path to your JSON configuration file\n",
        "config_file_path = 'config.json'\n",
        "\n",
        "with open(config_file_path, 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "CONNECTION_STRING_AZURE_STORAGE = config['connectionString']\n",
        "CONTAINER_AZURE = 'studentattendance'\n",
        "\n",
        "# Create the BlobServiceClient object\n",
        "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)\n",
        "\n",
        "# Iterate over DataFrame chunks and upload each chunk as a CSV file\n",
        "for i, df_chunk in enumerate(dfs):\n",
        "    # Convert DataFrame chunk to CSV string\n",
        "    output = StringIO()\n",
        "    df_chunk.to_csv(output, index=False)\n",
        "    data = output.getvalue()\n",
        "    output.close()\n",
        "\n",
        "    # Define the blob name for the current chunk\n",
        "    blob_name = f'nycatt_chunk_{i+1}.csv'\n",
        "\n",
        "    # Get a blob client using the container name and blob name\n",
        "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_AZURE, blob=blob_name)\n",
        "\n",
        "    # Upload the CSV data\n",
        "    blob_client.upload_blob(data, overwrite=True)"
      ],
      "metadata": {
        "id": "yh6R-ENYWPuH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79hDPT-mYFTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    output = StringIO()\n",
        "    df_raw.to_csv(output, index=False)\n",
        "    data = output.getvalue()\n",
        "    output.close()\n",
        "\n",
        "    # Azure\n",
        "    azure_connect_str = 'YOUR_AZURE_STORAGE_CONNECTION_STRING'\n",
        "    azure_container_name = 'YOUR_CONTAINER_NAME'\n",
        "    azure_blob_name = 'your_blob_name.csv'\n",
        "    azure_data = 'Hello, Azure!'\n",
        "    azure_upload_blob(azure_connect_str, azure_container_name, azure_blob_name, azure_data)\n",
        "    # Download and print the blob content\n",
        "    azure_blob_content = azure_download_blob(azure_connect_str, azure_container_name, azure_blob_name)\n",
        "    print(azure_blob_content)"
      ],
      "metadata": {
        "id": "OaXloXP3hcx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the path to your JSON configuration file\n",
        "config_file_path = 'config.json'\n",
        "\n",
        "#load the JSON configuration file\n",
        "with open(config_file_path, 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "#print the configuration\n",
        "#Connection_STRING = config['CONNECTION_STRING_AZURE_STORAGE']\n",
        "\n",
        "CONNECTION_STRING_AZURE_STORAGE = config['connectionString']\n",
        "CONTAINER_AZURE = 'studentattendance'\n",
        "blob_name = 'nycatt.csv'\n",
        "\n",
        "# Convert DataFrame to CSV\n",
        "output = StringIO()\n",
        "df_raw.to_csv(output, index=False)\n",
        "data = output.getvalue()\n",
        "output.close()\n",
        "\n",
        "# Create the BlobServiceClient object\n",
        "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)\n",
        "\n",
        "# Get a blob client using the container name and blob name\n",
        "blob_client = blob_service_client.get_blob_client(container=CONTAINER_AZURE, blob=blob_name)\n",
        "\n",
        "# Upload the CSV data\n",
        "blob_client.upload_blob(data, overwrite=True)\n",
        "\n",
        "print(f\"Uploaded {blob_name} to Azure Blob Storage in container {CONTAINER_AZURE}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjCRbCCnNUdq",
        "outputId": "bd51e07b-cc8d-41f3-85d6-36137518a34f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded nycatt.csv to Azure Blob Storage in container studentattendance.\n"
          ]
        }
      ]
    }
  ]
}