{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gicUEe1hOBDP"
      },
      "outputs": [],
      "source": [
        "!pip install azure-storage-blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DdJ-2JsVG3c6"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import requests\n",
        "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUvaAkSoJZ6m"
      },
      "source": [
        "s1 --> Gather --> Decompress --> Convert to Tabular --> Clean --> Reformat --> Consolidate --> Tranformation --> Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NuuN9M7tSSVn"
      },
      "outputs": [],
      "source": [
        "# Function\n",
        "\n",
        "import os\n",
        "import boto3\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "from io import BytesIO, StringIO\n",
        "\n",
        "# Azure Functions\n",
        "def azure_upload_blob(connect_str, container_name, blob_name, data):\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    blob_client.upload_blob(data, overwrite=True)\n",
        "    print(f\"Uploaded to Azure Blob: {blob_name}\")\n",
        "\n",
        "def azure_download_blob(connect_str, container_name, blob_name):\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
        "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "    download_stream = blob_client.download_blob()\n",
        "    return download_stream.readall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCjjD6utNLrq"
      },
      "source": [
        "#Chunking data from API into list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TGx3lO9nVqKB"
      },
      "outputs": [],
      "source": [
        "from sodapy import Socrata\n",
        "import pandas as pd\n",
        "data_url='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set='vww9-qguh'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token='m93ORKHDVAEgZJ48OfuDNs5sI'   # The app token created in the prior steps\n",
        "client = Socrata(data_url, app_token)\n",
        "client.timeout = 60\n",
        "\n",
        "# Define the chunk size\n",
        "chunk_size = 50000\n",
        "offset = 0\n",
        "dfs = []\n",
        "\n",
        "while True:\n",
        "    # Retrieve data for the current chunk\n",
        "    results = client.get(data_set, limit=chunk_size, offset=offset)\n",
        "\n",
        "    # If there are no more results, break out of the loop\n",
        "    if not results:\n",
        "        break\n",
        "\n",
        "    # Convert the chunk to a DataFrame and append it to the list of DataFrames\n",
        "    df_chunk = pd.DataFrame.from_records(results)\n",
        "    dfs.append(df_chunk)\n",
        "\n",
        "    # Increment the offset for the next chunk\n",
        "    offset += chunk_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU50TeROXrbY"
      },
      "source": [
        "#Creating connection to Azure and uploading chunks into Azure Blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yh6R-ENYWPuH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from io import StringIO\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "# Assuming you already have the DataFrame chunks stored in 'dfs' list\n",
        "#specify the path to your JSON configuration file\n",
        "config_file_path = 'config.json'\n",
        "\n",
        "with open(config_file_path, 'r') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "CONNECTION_STRING_AZURE_STORAGE = config['connectionString']\n",
        "CONTAINER_AZURE = 'studentattendance'\n",
        "\n",
        "# Create the BlobServiceClient object\n",
        "blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)\n",
        "\n",
        "# Iterate over DataFrame chunks and upload each chunk as a CSV file\n",
        "for i, df_chunk in enumerate(dfs):\n",
        "    # Convert DataFrame chunk to CSV string\n",
        "    output = StringIO()\n",
        "    df_chunk.to_csv(output, index=False)\n",
        "    data = output.getvalue()\n",
        "    output.close()\n",
        "\n",
        "    # Define the blob name for the current chunk\n",
        "    blob_name = f'nycatt_chunk_{i+1}.csv'\n",
        "\n",
        "    # Get a blob client using the container name and blob name\n",
        "    blob_client = blob_service_client.get_blob_client(container=CONTAINER_AZURE, blob=blob_name)\n",
        "\n",
        "    # Upload the CSV data\n",
        "    blob_client.upload_blob(data, overwrite=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
